{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import mnist_utils\n",
    "import utils\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "NUM_TEST_IMAGES = 10000\n",
    "\n",
    "def input_fn(mode, hparams):\n",
    "    \"\"\"Get input tensors\"\"\"\n",
    "\n",
    "    # Load data into memory\n",
    "    dataset_path = '../data/mnist/'\n",
    "    mnist = input_data.read_data_sets(dataset_path, one_hot=True)\n",
    "\n",
    "    if mode == 'TRAIN':\n",
    "        min_queue_examples = 20000\n",
    "        batch_size = hparams.train_batch_size\n",
    "        input_, target = tf.train.shuffle_batch([mnist.train.images, mnist.train.labels],\n",
    "                                                batch_size,\n",
    "                                                capacity=min_queue_examples + 3*batch_size,\n",
    "                                                min_after_dequeue=min_queue_examples,\n",
    "                                                enqueue_many=True)        \n",
    "    elif mode == 'TEST':\n",
    "        input_, target = tf.train.batch(\n",
    "            [mnist.test.images, mnist.test.labels], hparams.test_batch_size, enqueue_many=True)\n",
    "\n",
    "    return input_, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hparams():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(input_, logits):\n",
    "    assert(input_.get_shape() == logits.get_shape())\n",
    "    all_losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, input_)\n",
    "    loss = tf.reduce_mean(all_losses, name='loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(input_, mode, hparams):\n",
    "    if mode == 'TEST':\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "    A_scale = 1.0 / hparams.num_measurements\n",
    "    A_init = np.random.normal(size=[784, hparams.num_measurements], scale=A_scale).astype(np.float32)\n",
    "    A = tf.get_variable('A', initializer=A_init, trainable=hparams.is_A_trainable)\n",
    "    \n",
    "    y = tf.matmul(input_, A)\n",
    "    hidden = y\n",
    "    prev_hidden_size = hparams.num_measurements\n",
    "    for i, hidden_size in enumerate(hparams.layer_sizes):\n",
    "        layer_name = 'hidden{0}'.format(i)\n",
    "        with tf.variable_scope(layer_name):\n",
    "            weights = tf.get_variable('weights', shape=[prev_hidden_size, hidden_size])\n",
    "            biases = tf.get_variable('biases', initializer =tf.zeros([hidden_size]))\n",
    "            hidden = tf.nn.relu(tf.matmul(hidden, weights) + biases, name=layer_name)\n",
    "        prev_hidden_size = hidden_size\n",
    "        \n",
    "    with tf.variable_scope('sigmoid_logits'):\n",
    "        weights = tf.get_variable('weights', shape=[prev_hidden_size, 784])\n",
    "        biases = tf.get_variable('biases', initializer =tf.zeros([784]))\n",
    "        logits = tf.add(tf.matmul(hidden, weights), biases, name='logits')\n",
    "        \n",
    "    prediction = tf.nn.sigmoid(logits)\n",
    "    loss = get_loss(input_, logits)\n",
    "        \n",
    "    return loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hparams = Hparams()\n",
    "hparams.dataset = 'mnist'\n",
    "hparams.train_batch_size = 64\n",
    "hparams.test_batch_size = 10\n",
    "hparams.num_measurements = 30\n",
    "hparams.n_input = 784\n",
    "hparams.layer_sizes = [50, 200]\n",
    "\n",
    "hparams.learning_rate = 0.001\n",
    "hparams.momentum = 0.9\n",
    "hparams.optimizer_type = 'adam'\n",
    "\n",
    "hparams.measurement_type = 'learned'\n",
    "hparams.model_types = ['e2e']\n",
    "hparams.image_matrix = 1\n",
    "\n",
    "hparams.summary_iter = 20\n",
    "hparams.checkpoint_iter = 2000\n",
    "\n",
    "hparams.max_train_epochs = 50\n",
    "hparams.num_train_examples_per_epoch = 55000\n",
    "# hparams.max_train_steps = hparams.max_train_epochs * hparams.num_train_examples_per_epoch / hparams.train_batch_size\n",
    "hparams.max_train_steps = 50000\n",
    "\n",
    "hparams.is_A_trainable = True\n",
    "\n",
    "base_dir_pattern = '../optimization/mnist-e2e/{0}/'\n",
    "dirname = '{0}_{1}_{2}_{3}/'.format(hparams.optimizer_type,\n",
    "                                    hparams.learning_rate,\n",
    "                                    hparams.num_measurements,\n",
    "                                    hparams.is_A_trainable)\n",
    "\n",
    "hparams.summary_dir = base_dir_pattern.format('summaries') + dirname\n",
    "hparams.checkpoint_dir = base_dir_pattern.format('checkpoints') + dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input, _ = input_fn('TRAIN', hparams)\n",
    "test_input, _ = input_fn('TEST', hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "train_loss, train_prediction = model_fn(train_input, 'TRAIN', hparams)\n",
    "test_loss, test_prediction = model_fn(test_input, 'TEST', hparams)\n",
    "train_summary = tf.summary.scalar('train_loss', train_loss)\n",
    "\n",
    "# Set up the solver\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "opt = utils.get_optimizer(hparams.learning_rate, hparams)\n",
    "train_op = opt.minimize(train_loss, global_step=global_step, name='train_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# Summary writer setup\n",
    "utils.set_up_dir(hparams.summary_dir)\n",
    "summary_writer = tf.summary.FileWriter(hparams.summary_dir, sess.graph)\n",
    "\n",
    "# Model checkpointing setup\n",
    "utils.set_up_dir(hparams.checkpoint_dir)\n",
    "\n",
    "model_saver = tf.train.Saver(tf.global_variables() + tf.local_variables())\n",
    "\n",
    "# Load variables from checkpoint or pretrained model\n",
    "ckpt_path = utils.get_checkpoint_path(hparams.checkpoint_dir)\n",
    "if ckpt_path:  # if a previous checkpoint exists\n",
    "    model_saver.restore(sess, ckpt_path)\n",
    "    ckpt_global_step = int(ckpt_path.split('/')[-1].split('-')[-1])\n",
    "    print 'Succesfully loaded model from {0} at step = {1}'.format(\n",
    "        ckpt_path, ckpt_global_step)\n",
    "    train_start_step = ckpt_global_step\n",
    "else:\n",
    "    train_start_step = 0\n",
    "    init_ops = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "    sess.run(init_ops)\n",
    "        \n",
    "    \n",
    "for train_step in range(train_start_step+1, hparams.max_train_steps):\n",
    "    _ = sess.run([train_op])\n",
    " \n",
    "    if train_step % hparams.summary_iter == 0:\n",
    "        train_summary_str = sess.run(train_summary)    \n",
    "        summary_writer.add_summary(train_summary_str, train_step)\n",
    "\n",
    "    if train_step % 1000 == 0:\n",
    "        train_loss_val = sess.run([train_loss])\n",
    "        test_input_val, test_pred_val, test_loss_val = sess.run([test_input, test_prediction, test_loss])\n",
    "        xs_dict = {i : test_input_val[i, :] for i in range(10)}\n",
    "        x_hats_dict = {'e2e' : {i : test_pred_val[i, :] for i in range(10)}}\n",
    "        print 'Train step = {0}, Train Loss = {1}, Test Loss = {2}'.format(train_step, \n",
    "                                                                           train_loss_val,\n",
    "                                                                           test_loss_val)\n",
    "        print sess.run(tf.global_variables()[7])\n",
    "        utils.image_matrix(xs_dict, x_hats_dict, mnist_utils.view_image, hparams)    \n",
    "\n",
    "    # Checkpointing\n",
    "    if train_step % hparams.checkpoint_iter == 0:\n",
    "        model_saver.save(sess, hparams.checkpoint_dir + 'snapshot.ckpt', global_step=train_step)\n",
    "\n",
    "# Final checkpoint\n",
    "model_saver.save(sess, hparams.checkpoint_dir + 'snapshot.ckpt', global_step=hparams.max_train_steps-1)\n",
    "        \n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained model and run it on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "model_saver = tf.train.Saver(tf.global_variables() + tf.local_variables())\n",
    "\n",
    "# Load variables from checkpoint or pretrained model\n",
    "ckpt_path = utils.get_checkpoint_path(hparams.checkpoint_dir)\n",
    "if ckpt_path:  # if a previous checkpoint exists\n",
    "    model_saver.restore(sess, ckpt_path)\n",
    "    ckpt_global_step = int(ckpt_path.split('/')[-1].split('-')[-1])\n",
    "    print 'Succesfully loaded model from {0} at step = {1}'.format(\n",
    "        ckpt_path, ckpt_global_step)\n",
    "    train_start_step = ckpt_global_step\n",
    "else:\n",
    "    train_start_step = 0\n",
    "    init_ops = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "    sess.run(init_ops)\n",
    "        \n",
    "    \n",
    "for _ in range(1):\n",
    "    train_loss_val = sess.run(train_loss)\n",
    "    test_input_val, test_pred_val, test_loss_val = sess.run([test_input, test_prediction, test_loss])\n",
    "    xs_dict = {i : test_input_val[i, :] for i in range(10)}\n",
    "    x_hats_dict = {'e2e' : {i : test_pred_val[i, :] for i in range(10)}}\n",
    "    print 'Train Loss = {0}, Test Loss = {1}'.format(train_loss_val, test_loss_val)\n",
    "    print sess.run(tf.global_variables()[7])\n",
    "    utils.image_matrix(xs_dict, x_hats_dict, mnist_utils.view_image, hparams)    \n",
    "    \n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
